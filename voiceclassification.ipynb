{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d2712-9aff-4ad0-a024-9a5a66a8ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision.datasets.utils import download_url\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tarfile\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932831d1-b4e5-4440-b17a-b4dd08817517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import csv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034638e-ac26-47d8-8f45-125fe41a2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(f\"file directory\")  #enter dataset directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96feca5e-76a6-4163-8c36-96efad381b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "for x in digit:\n",
    "    print(x, \": \", len(os.listdir('/content/data/'+x)))\n",
    "#this is for balancing and for informations about our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845dbd1-2279-4f29-8659-6c960973cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "d = {}\n",
    "with open(\"Spoken_digit.csv\", 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"File\", \"Label\"])\n",
    "    for x in digit:\n",
    "        if os.path.isdir('/content/data/'+x):\n",
    "            d[x] = os.listdir('/content/data/'+x)\n",
    "            for name in os.listdir('/content/data/'+x):\n",
    "                if os.path.isfile('/content/data/'+x+\"/\"+name):\n",
    "                    csvwriter.writerow([x+'/'+name, x])\n",
    "\n",
    "#shuffle \n",
    "df = pd.read_csv('Spoken_digit.csv')\n",
    "df = df.sample(frac=1)\n",
    "df.to_csv('Spoken_digit.csv', index = False)\n",
    "\n",
    "dfx = pd.DataFrame.from_dict(d, orient='index', dtype = 'float32').transpose()\n",
    "dfx.to_csv('Spoken_digit_X.csv', index = False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b981b0-7d74-4c74-9788-8390da1794c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main part for feature extraction\n",
    "\n",
    "def extract_features(path):\n",
    "    data, sr = librosa.load('/content/data/'+path)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y = data, sr=sr).T, axis = 0)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(data+0.01, sr=sr)[0]\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S = stft, sr = sr).T, axis = 0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(data, sr).T, axis = 0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S = stft, sr = sr).T, axis = 0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y = librosa.effects.harmonic(data), sr = sr).T, axis = 0)\n",
    "    \n",
    "    #print(mfccs.shape, spectral_centroids.shape, stft.shape, chroma.shape, mel.shape, contrast.shape, tonnetz.shape)\n",
    "    \n",
    "    #spectral_centroids have varying shapes for each datapoint and stft is 2d array. Thus they are not included in the final features.\n",
    "    return np.concatenate((mfccs, chroma, mel, contrast, tonnetz), axis = 0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf575d1-afaf-449b-9b18-49993bc280bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigist(Dataset):\n",
    "    def __init__(self, file = None, rootdir = None):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.rootdir = rootdir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.loc[i]\n",
    "        fname, label = row['File'], row['Label']\n",
    "        fts = extract_features(fname)\n",
    "        #print(len(fts))\n",
    "        return torch.tensor(fts), torch.tensor(digit.index(label))\n",
    "    \n",
    "    def getsr(self, i):\n",
    "        fname, label = row['File'], row['Label']\n",
    "        _, sr = librosa.load(self.rootdir+'/'+fname)\n",
    "        return sr\n",
    "    \n",
    "#Labeling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e36c-9190-489c-b752-802abe197f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_dset = SpokenDigit(file = \"Spoken_digit.csv\", rootdir = \"/content/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752018e3-c9ad-454d-97ae-e3142a9b92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after we extracted features we must put features into our model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f89ad3-591e-4118-8b86-c1ffe46d363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tarfile\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c941e0-30e9-4795-b8b8-e805d1109861",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fffe1-ea5a-4679-8ffc-bcc36af9e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spoken_mnist_finalfts.csv is generated in last part(feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbdd6ab-8d95-474d-af99-d4c4f13d1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalfts = pd.read_csv(\"Spoken_digit_finalfts.csv\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "finalfts[finalfts.columns[1:]] = scale.fit_transform(finalfts[finalfts.columns[1:]])\n",
    "\n",
    "spokendset = TensorDataset(torch.tensor(np.array(finalfts[finalfts.columns[1:]].astype('float32'))),torch.tensor(finalfts['Label'])) \n",
    "\n",
    "# 90-10 split\n",
    "size = len(spokendset)\n",
    "val_size = int(0.1 * size)\n",
    "train_size = size - val_size \n",
    "\n",
    "train_dset, val_dset = random_split(spokendset, [train_size, val_size])\n",
    "\n",
    "train_size, val_size\n",
    "\n",
    "train_dl = DataLoader(train_dset, 512, True)\n",
    "val_dl = DataLoader(val_dset, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c91804-deac-4bab-9aa3-a6c25b199cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ec2d0-7e0b-426f-a6e2-e69a060784c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "val_dl = DeviceDataLoader(val_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8e0c3-81a7-42de-ae85-4376ce1390e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train part************************************************************\n",
    "\n",
    "class SpokenDigitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(173, 1024)\n",
    "        self.l2 = nn.Linear(1024, 512)\n",
    "        self.l3 = nn.Linear(512, 64)\n",
    "        self.l4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        accuracy = torch.tensor(torch.sum(pred==labels).item()/len(pred))\n",
    "        return [loss.detach(), accuracy.detach()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b455e-d3b5-400b-bc53-f7e46eae4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in loader]\n",
    "    outputs = torch.tensor(outputs).T\n",
    "    loss, accuracy = torch.mean(outputs, dim=1)\n",
    "    return {\"loss\" : loss.item(), \"accuracy\" : accuracy.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156de602-3d2b-43ff-9bca-6736a7173f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5485958-bc37-4dd7-96c4-22dad9b5bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, epochs, lr, optimizer_function = torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer = optimizer_function(model.parameters(), lr)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \", epoch)\n",
    "        #Train\n",
    "        model.train()\n",
    "        lrs = []\n",
    "        tr_loss = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            tr_loss.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        #Validate\n",
    "        result = evaluate(model, val_loader)\n",
    "        result[\"lrs\"] = lrs\n",
    "        result[\"train loss\"] = torch.stack(tr_loss).mean().item()\n",
    " \n",
    "        print(\"Last lr: \", lrs[-1],\" Train_loss: \", result[\"train loss\"], \" Val_loss: \", result['loss'], \" Accuracy: \", result['accuracy'])\n",
    "        history.append(result)         \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee999d5c-35fc-4085-ac15-120e912bee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(SpokenDigitModel(), device)\n",
    "history = []\n",
    "evaluate(model, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08f425-dbf8-4474-8a0c-085656590076",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append(fit(model, train_dl, val_dl, 64, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f59c59-88f1-4329-9089-a9ce23c7cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_dl(model, dl):\n",
    "    torch.cuda.empty_cache()\n",
    "    batch_probs = []\n",
    "    batch_targ = []\n",
    "    for xb, yb in dl:\n",
    "        probs = model(xb)\n",
    "        batch_probs.append(probs.cpu().detach())\n",
    "        batch_targ.append(yb.cpu().detach())\n",
    "    batch_probs = torch.cat(batch_probs)\n",
    "    batch_targ = torch.cat(batch_targ)\n",
    "    return [list(values).index(max(values)) for values in batch_probs], batch_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e9697-59f7-4448-8cd7-36651db5afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = evaluate(model, val_dl)\n",
    "yp, yt = predict_dl(model, val_dl)\n",
    "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'], \"\\nF-score: \", f1_score(yt, yp, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
